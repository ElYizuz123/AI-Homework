{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de838f86-bd53-4daf-a839-825121461b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuy-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\chuy-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd15f4d-b821-4a55-8b88-d827a5f2bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Cargar dataset\n",
    "# -----------------------------\n",
    "dataset = load_dataset(\"json\", data_files=\"dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7319922-23b2-4153-bb50-2d79e598375a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or 'auto', 'balanced', 'balanced_low_0', 'sequential' but found None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:4788\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4787\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, maia, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: None",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/Phi-3.5-mini-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    605\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    606\u001b[0m     )\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    610\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:4790\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4788\u001b[0m         device_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mdevice(device_map)}\n\u001b[0;32m   4789\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[1;32m-> 4790\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   4791\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4792\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced_low_0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequential\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_map\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4793\u001b[0m         )\n\u001b[0;32m   4794\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m   4795\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or 'auto', 'balanced', 'balanced_low_0', 'sequential' but found None."
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 2. Cargar modelo base\n",
    "# -----------------------------\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"None\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff696da2-51df-4d52-8876-2658cbeae492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Configurar LoRA\n",
    "# -----------------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70515249-2533-4189-9c7b-84399918d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Preprocesar el dataset\n",
    "# -----------------------------\n",
    "def format_instruction(example):\n",
    "    prompt = f\"Instrucci√≥n: {example['instruction']}\\nRespuesta:\"\n",
    "    return tokenizer(prompt + example[\"response\"], truncation=True)\n",
    "\n",
    "tokenized = dataset.map(format_instruction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0f62a96-d18c-4a68-b9d8-270c529683e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuy-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\training_args.py:1636: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function MmBackward0 returned an invalid gradient at index 1 - expected device meta but got cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m\n\u001b[0;32m      4\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      5\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./lora-tutor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     no_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,           \u001b[38;5;66;03m# IMPORTANT√çSIMO\u001b[39;00m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     17\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     18\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     19\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     20\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2672\u001b[0m )\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2680\u001b[0m ):\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:4071\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   4069\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 4071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\accelerator.py:2852\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2850\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2851\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2852\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    624\u001b[0m     )\n\u001b[1;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    843\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Function MmBackward0 returned an invalid gradient at index 1 - expected device meta but got cpu"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5. Entrenamiento\n",
    "# -----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-tutor\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=1,     # Pon 1 para no tardar horas\n",
    "    fp16=False,             # IMPORTANTE\n",
    "    bf16=False,\n",
    "    save_steps=500,\n",
    "    no_cuda=True,           # IMPORTANT√çSIMO\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38256eda-fd89-43fb-b334-552c87e83397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Guardar adaptadores LoRA\n",
    "# -----------------------------\n",
    "model.save_pretrained(\"./lora-tutor\")\n",
    "print(\"Entrenamiento completado. Adaptadores guardados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "105fa2c4-0eba-4c4f-9459-db262bcd0b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:20<00:00, 10.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 3,825,798,144 || trainable%: 0.1233\n",
      "Par√°metros entrenables: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 79/79 [00:00<00:00, 459.39 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento en CPU (esto puede ser LENTO)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuy-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 3:38:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.105000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Entrenamiento completado. Adaptadores guardados en ./lora-tutor\n",
      "\n",
      "--- Prueba de inferencia ---\n",
      "¬øC√≥mo crear una lista en Python? Crear una lista en Python es muy simple y directo. Aqu√≠ tienes una gu√≠a paso a paso y un ejemplo de c√≥mo crear una lista:\n",
      "\n",
      "1. **Inicializar una lista vac√≠a**: Puedes crear una lista vac√≠a usando corchetes `[]`.\n",
      "\n",
      "```python\n",
      "mi_lista = []\n",
      "```\n",
      "\n",
      "2. **Agregar elementos a la lista**: Puedes agregar\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Cargar dataset\n",
    "# -----------------------------\n",
    "dataset = load_dataset(\"json\", data_files=\"tutor_dataset.jsonl\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Cargar modelo base (CPU optimizado)\n",
    "# -----------------------------\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Cargar modelo en CPU con dtype float32 (sin cuantizaci√≥n)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,  # Usar float32 para CPU\n",
    "    low_cpu_mem_usage=True,     # Optimizar uso de memoria\n",
    "    device_map=\"cpu\"            # Forzar CPU\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Configurar LoRA (ajustado para Phi-3.5)\n",
    "# -----------------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                        # Phi-3.5 mini es peque√±o, podemos usar r=8\n",
    "    lora_alpha=16,              \n",
    "    target_modules=[\"qkv_proj\", \"o_proj\"],  # M√≥dulos correctos para Phi-3.5\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(f\"Par√°metros entrenables: {model.print_trainable_parameters()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Preprocesar el dataset\n",
    "# -----------------------------\n",
    "def format_instruction(example):\n",
    "    # Formato espec√≠fico para Phi-3.5\n",
    "    prompt = f\"<|user|>\\n{example['instruction']}<|end|>\\n<|assistant|>\\n\"\n",
    "    full_text = prompt + example[\"response\"] + \"<|end|>\"\n",
    "    \n",
    "    # Tokenizar con longitud m√°xima\n",
    "    return tokenizer(\n",
    "        full_text, \n",
    "        truncation=True, \n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    format_instruction, \n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Entrenamiento (configuraci√≥n para CPU)\n",
    "# -----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-tutor\",\n",
    "    per_device_train_batch_size=2,      # Phi-3.5 es m√°s peque√±o, podemos usar batch_size=2\n",
    "    gradient_accumulation_steps=8,      \n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,                 # 3 √©pocas es razonable para Phi-3.5\n",
    "    fp16=False,                         \n",
    "    save_steps=100,\n",
    "    save_total_limit=2,                 \n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=50,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    dataloader_num_workers=0,           \n",
    "    optim=\"adamw_torch\",                \n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",                   \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "print(\"Iniciando entrenamiento en CPU (esto puede ser LENTO)...\")\n",
    "trainer.train()\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Guardar adaptadores LoRA\n",
    "# -----------------------------\n",
    "model.save_pretrained(\"./lora-tutor\")\n",
    "tokenizer.save_pretrained(\"./lora-tutor\")\n",
    "print(\"‚úÖ Entrenamiento completado. Adaptadores guardados en ./lora-tutor\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Ejemplo de inferencia (opcional)\n",
    "# -----------------------------\n",
    "print(\"\\n--- Prueba de inferencia ---\")\n",
    "model.eval()\n",
    "test_prompt = \"<|user|>\\n¬øC√≥mo crear una lista en Python?<|end|>\\n<|assistant|>\\n\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee4ff3cf-3ea7-4297-a8bc-86e51dc0b1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:24<00:00, 12.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando adaptadores LoRA...\n",
      "\n",
      "============================================================\n",
      "ü§ñ Tutor de Programaci√≥n LoRA - Listo para responder\n",
      "============================================================\n",
      "\n",
      "Pregunta: Expl√≠came qu√© es una funci√≥n en Python.\n",
      "Respuesta: Expl√≠came qu√© es una funci√≥n en Python. En el contexto del lenguaje de programaci√≥n Python, la palabra \"funci√≥n\" se refiere a un bloque compuesto y reutilizable de c√≥digo que realiza tareas espec√≠ficas o calculos predefinidos cuando se llama con ciertos argumentos (llamados par√°metros). Las funciones ayudan al desarrollo eficiente dividiendo las operaciones complejas en partes m√°s peque√±as y manejables, permitiendo as√≠ mejor legibilidad, modularizaci√≥n y mantenimiento del programa.\n",
      "\n",
      "Aqu√≠ hay algunas caracter√≠sticas clave sobre las funciones en Python:\n",
      "\n",
      "1. Definici√≥n: Una funci√≥n puede definirse usando la sintaxis `def` seguida por su nombre junto con los par√©ntesis para contener sus posibles argumentos entre ellos. Despu√©s viene dos puntos (:) y luego la lista de declaraciones dentro de l√≠neas separadas u otra estructura como indentado ra√≠z. Los comentarios pueden incluirse tambi√©n antes (`#`) de esta l√≠nea. Por ejemplo:\n",
      "```python\n",
      "def mi_funcion(parametro):  # Declara tu propia funcion aqu√≠\n",
      "    print(\"El valor pasado fue\", parametro)\n",
      "```\n",
      "2. Argumentos Opcionales/Valores Predeterminados: Puede proporcionar valores predeterminados para algunos de los par\n",
      "\n",
      "Pregunta: ¬øC√≥mo puedo leer un archivo en Python?\n",
      "Respuesta: ¬øC√≥mo puedo leer un archivo en Python? Para leer contenido de un archivo usando Python, puede usar el m√©todo `open()` junto con una instrucci√≥n `with`. Aqu√≠ hay un ejemplo simple:\n",
      "\n",
      "```python\n",
      "try:\n",
      "    with open('yourfile.txt', 'r') as file:  # reemplaza 'yourfile.txt' por la ruta/nombre real del tuyo archivo\n",
      "        content = file.read()\n",
      "except FileNotFoundError:\n",
      "    print(\"El archivo no fue encontrado.\")\n",
      "else:\n",
      "    print(content)\n",
      "```\n",
      "\n",
      "En este c√≥digo, primero intentamos abrir el archivo especificado utilizando `'r'` como modo (para lectura). La declaraci√≥n `with` asegura que se cierre autom√°ticamente y libere los recursos despu√©s de terminar nuestro bloque anidado sin necesidad de llamadas expl√≠citas al `.close()`. Luego, accedemos al contenido del archivo mediante el atributo `.read()`, lo cual guarda toda la informaci√≥n dentro del variable `content`. Finalmente, imprimimos ese contenido o manejamos cualquier error potencial relacionado con falta de archivos (`FileNotFoundError`). Ajuste las partes relevantes seg√∫n su caso particular para trabajar eficazmente con sus propios datos.\n",
      "\n",
      "Pregunta: ¬øQu√© es un bucle for?\n",
      "Respuesta: ¬øQu√© es un bucle for? Un \"bucle for\" (o simplemente 'for') se refiere a una estructura de control en programaci√≥n que permite ejecutar repetidamente un bloque de c√≥digo mientras se cumple cierta condici√≥n. Es particularmente √∫til cuando necesitas iterar sobre elementos dentro de colecciones, como listas, conjuntos o diccionarios.\n",
      "\n",
      "Aqu√≠ tienes la forma b√°sica y funcionalidad del bucle `for` en Python:\n",
      "\n",
      "```python\n",
      "para elemento en collection_name:  # La sintaxis para el bucle for general\n",
      "    instrucci√≥n(s)           # El conjunto de comandos/instrucciones por realizar con cada iteraci√≥n\n",
      "```\n",
      "\n",
      "Ejemplo pr√°ctico usando lista en Python:\n",
      "\n",
      "```python\n",
      "mi_lista = [10, 20, 30, 40, 50]\n",
      "para valor in mi_lista:              # Iteramos sobre los valores en la lista\n",
      "    print(\"El n√∫mero actual es \", valor)   # Imprimimos el valor durante cada ciclo\n",
      "```\n",
      "\n",
      "Salida esperada:\n",
      "```\n",
      "El n√∫mero actual es 10\n",
      "El n√∫mero actual es 20\n",
      "El n√∫mero actual es 30\n",
      "El n√∫mero actual es 40\n",
      "El n√∫mero actual es 50\n",
      "```\n",
      "En este ejemplo, as\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Configuraci√≥n\n",
    "# -----------------------------\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\" \n",
    "lora_path = \"./lora-tutor\"  \n",
    "\n",
    "# -----------------------------\n",
    "# 2. Cargar modelo base y adaptadores LoRA\n",
    "# -----------------------------\n",
    "print(\"Cargando modelo base...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float32,      \n",
    "    device_map=\"cpu\",               \n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"Cargando adaptadores LoRA...\")\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "model.eval()  # Modo evaluaci√≥n\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Funci√≥n de inferencia\n",
    "# -----------------------------\n",
    "def generar_respuesta(pregunta, max_tokens=300, temperature=0.7):\n",
    "    # Formato correcto para Phi-3.5\n",
    "    prompt = f\"<|user|>\\n{pregunta}<|end|>\\n<|assistant|>\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    # NO usar .to(\"cuda\") porque estamos en CPU\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    respuesta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extraer solo la respuesta del asistente\n",
    "    if \"<|assistant|>\" in respuesta_completa:\n",
    "        respuesta = respuesta_completa.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        respuesta = respuesta_completa\n",
    "    \n",
    "    return respuesta\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Ejemplos de uso\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ü§ñ Tutor de Programaci√≥n LoRA - Listo para responder\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Ejemplo 1\n",
    "pregunta1 = \"Expl√≠came qu√© es una funci√≥n en Python.\"\n",
    "print(f\"Pregunta: {pregunta1}\")\n",
    "print(f\"Respuesta: {generar_respuesta(pregunta1)}\\n\")\n",
    "\n",
    "# Ejemplo 2\n",
    "pregunta2 = \"¬øC√≥mo puedo leer un archivo en Python?\"\n",
    "print(f\"Pregunta: {pregunta2}\")\n",
    "print(f\"Respuesta: {generar_respuesta(pregunta2)}\\n\")\n",
    "\n",
    "# Ejemplo 3\n",
    "pregunta3 = \"¬øQu√© es un bucle for?\"\n",
    "print(f\"Pregunta: {pregunta3}\")\n",
    "print(f\"Respuesta: {generar_respuesta(pregunta3)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927689a-d7b4-48d1-866f-124195901256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845335d8-d2c0-411f-a767-4ec86608d3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuy-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:15<00:00,  7.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando adaptadores LoRA...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ Tu pregunta:  ¬øQu√© es un ciclo for? y dame un ejemplo de java \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Configuraci√≥n\n",
    "# -----------------------------\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\" \n",
    "lora_path = \"./lora-tutor\"  \n",
    "\n",
    "# -----------------------------\n",
    "# 2. Cargar modelo base y adaptadores LoRA\n",
    "# -----------------------------\n",
    "print(\"Cargando modelo base...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float32,      \n",
    "    device_map=\"cpu\",               \n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"Cargando adaptadores LoRA...\")\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "model.eval()  # Modo evaluaci√≥n\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Funci√≥n de inferencia\n",
    "# -----------------------------\n",
    "def generar_respuesta(pregunta, max_tokens=50, temperature=0.7):\n",
    "    # Formato correcto para Phi-3.5\n",
    "    prompt = f\"<|user|>\\n{pregunta}<|end|>\\n<|assistant|>\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    # NO usar .to(\"cuda\") porque estamos en CPU\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    respuesta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extraer solo la respuesta del asistente\n",
    "    if \"<|assistant|>\" in respuesta_completa:\n",
    "        respuesta = respuesta_completa.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        respuesta = respuesta_completa\n",
    "    \n",
    "    return respuesta\n",
    "while True:\n",
    "    pregunta = input(\"\\nüë§ Tu pregunta: \").strip()\n",
    "    \n",
    "    if pregunta.lower() in ['salir', 'exit', 'quit']:\n",
    "        print(\"¬°Hasta luego! üëã\")\n",
    "        break\n",
    "    \n",
    "    if not pregunta:\n",
    "        continue\n",
    "    \n",
    "    print(f\"ü§ñ Respuesta: {generar_respuesta(pregunta)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e899ceb-7cc9-4c79-b90a-a3dab5122023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento LoRA con Qwen 2.5-0.5B\n",
      "======================================================================\n",
      "\n",
      "Cargando dataset...\n",
      " Dataset cargado: 500 ejemplos\n",
      "\n",
      "ü§ñ Cargando modelo: Qwen/Qwen2.5-0.5B-Instruct\n",
      "‚öôÔ∏è  Cargando modelo base...\n",
      " Modelo cargado en CPU con dtype float16\n",
      "\n",
      "üîß Configurando LoRA...\n",
      " LoRA configurado\n",
      "trainable params: 2,162,688 || all params: 496,195,456 || trainable%: 0.4359\n",
      "\n",
      "üìù Preprocesando dataset...\n",
      "‚úÖ Dataset tokenizado: 500 ejemplos\n",
      "\n",
      "‚öôÔ∏è  Configurando entrenamiento...\n",
      "\n",
      "üèãÔ∏è  Creando Trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " INICIANDO ENTRENAMIENTO\n",
      "======================================================================\n",
      "  ADVERTENCIA: En CPU esto puede tardar 30 min - 2 horas\n",
      "    dependiendo de tu procesador y tama√±o del dataset\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuy-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"Entrenamiento LoRA con Qwen 2.5-0.5B\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Cargar dataset\n",
    "# -----------------------------\n",
    "print(\"\\nCargando dataset...\")\n",
    "dataset = load_dataset(\"json\", data_files=\"tutor_dataset2.jsonl\")\n",
    "print(f\" Dataset cargado: {len(dataset['train'])} ejemplos\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Cargar modelo base LIGERO\n",
    "# -----------------------------\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "print(f\"\\nü§ñ Cargando modelo: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Configurar padding token si no existe\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"‚öôÔ∏è  Cargando modelo base...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,      \n",
    "    device_map=\"cpu\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Liberar memoria\n",
    "gc.collect()\n",
    "print(f\" Modelo cargado en CPU con dtype float16\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Configurar LoRA (optimizado para Qwen)\n",
    "# -----------------------------\n",
    "print(\"\\nüîß Configurando LoRA...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                           \n",
    "    lora_alpha=32,                  \n",
    "    target_modules=[\n",
    "        \"q_proj\", \n",
    "        \"k_proj\", \n",
    "        \"v_proj\", \n",
    "        \"o_proj\"\n",
    "    ],                              \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\" LoRA configurado\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Preprocesar el dataset\n",
    "# -----------------------------\n",
    "print(\"\\nüìù Preprocesando dataset...\")\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"\n",
    "    Formato de chat para Qwen 2.5:\n",
    "    <|im_start|>user\n",
    "    {pregunta}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    {respuesta}<|im_end|>\n",
    "    \"\"\"\n",
    "    prompt = f\"<|im_start|>user\\n{example['instruction']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    full_text = prompt + example[\"response\"] + \"<|im_end|>\"\n",
    "    \n",
    "    # Tokenizar con longitud controlada\n",
    "    return tokenizer(\n",
    "        full_text, \n",
    "        truncation=True, \n",
    "        max_length=512,             # L√≠mite razonable para CPU\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    format_instruction, \n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizando ejemplos\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset tokenizado: {len(tokenized['train'])} ejemplos\")\n",
    "\n",
    "# Liberar memoria\n",
    "gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Configuraci√≥n de entrenamiento\n",
    "# -----------------------------\n",
    "print(\"\\n‚öôÔ∏è  Configurando entrenamiento...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-qwen-tutor\",\n",
    "    per_device_train_batch_size=2,      \n",
    "    gradient_accumulation_steps=8,      \n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,                 \n",
    "    fp16=False,                         \n",
    "    bf16=False,                         \n",
    "    save_steps=100,\n",
    "    save_total_limit=2,                 \n",
    "    learning_rate=3e-4,                 \n",
    "    warmup_steps=50,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    dataloader_num_workers=0,           \n",
    "    optim=\"adamw_torch\",                \n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",                   \n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_strategy=\"no\",           \n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Crear Trainer\n",
    "# -----------------------------\n",
    "print(\"\\nüèãÔ∏è  Creando Trainer...\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. ENTRENAR\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" INICIANDO ENTRENAMIENTO\")\n",
    "print(\"=\"*70)\n",
    "print(\"  ADVERTENCIA: En CPU esto puede tardar 30 min - 2 horas\")\n",
    "print(\"    dependiendo de tu procesador y tama√±o del dataset\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"\\n Entrenamiento completado!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n  Entrenamiento interrumpido por el usuario\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n Error durante el entrenamiento: {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Guardar modelo\n",
    "# -----------------------------\n",
    "print(\"\\nüíæ Guardando adaptadores LoRA...\")\n",
    "model.save_pretrained(\"./lora-qwen-tutor\")\n",
    "tokenizer.save_pretrained(\"./lora-qwen-tutor\")\n",
    "print(\"‚úÖ Adaptadores guardados en: ./lora-qwen-tutor\")\n",
    "\n",
    "# Liberar memoria\n",
    "del model\n",
    "del trainer\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" PROCESO COMPLETADO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b03ea-a31e-4621-9173-dcc3585cf900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
