{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105fa2c4-0eba-4c4f-9459-db262bcd0b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 09:57:31.574513: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-11 09:57:31.612042: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-11 09:57:34.787543: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0d309e504c43efbbddda8f49a64cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45127fe2d6c4ff28c37395bc14cdf2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 3,825,798,144 || trainable%: 0.1233\n",
      "ParÃ¡metros entrenables: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c798238b2804abdb8ef2f7fddc4e5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento en CPU (esto puede ser LENTO)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yisus/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 5:34:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.367800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.415700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.930000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.841300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.806600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.770400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.802000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Entrenamiento completado. Adaptadores guardados en ./lora-tutor3\n",
      "\n",
      "--- Prueba de inferencia ---\n",
      "Â¿CÃ³mo crear una lista en Java? Las listas (ArrayList) son colecciones dinÃ¡micas que pueden expandirse de forma automatica.\n",
      "\n",
      "```java\n",
      "import java.util.ArrayList;\n",
      "\n",
      "public class Listas {\n",
      "    public static void main(String[] args) {\n",
      "        ArrayList<String> nombres = new ArrayList<>();\n",
      "        nombres.add(\"Alice\");\n",
      "        System.out.println(nombres.size()); // Salida: 1\n",
      "    }\n",
      "}```\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Cargar dataset\n",
    "# -----------------------------\n",
    "dataset = load_dataset(\"json\", data_files=\"tutor_dataset2.jsonl\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Cargar modelo base (CPU optimizado)\n",
    "# -----------------------------\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Cargar modelo en CPU con dtype float32 (sin cuantizaciÃ³n)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,  # Usar float32 para CPU\n",
    "    low_cpu_mem_usage=True,     # Optimizar uso de memoria\n",
    "    device_map=\"cpu\"            # Forzar CPU\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Configurar LoRA (ajustado para Phi-3.5)\n",
    "# -----------------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                        # Phi-3.5 mini es pequeÃ±o, podemos usar r=8\n",
    "    lora_alpha=16,              \n",
    "    target_modules=[\"qkv_proj\", \"o_proj\"],  # MÃ³dulos correctos para Phi-3.5\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(f\"ParÃ¡metros entrenables: {model.print_trainable_parameters()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Preprocesar el dataset\n",
    "# -----------------------------\n",
    "def format_instruction(example):\n",
    "    # Formato especÃ­fico para Phi-3.5\n",
    "    prompt = f\"<|user|>\\n{example['instruction']}<|end|>\\n<|assistant|>\\n\"\n",
    "    full_text = prompt + example[\"response\"] + \"<|end|>\"\n",
    "    \n",
    "    # Tokenizar con longitud mÃ¡xima\n",
    "    return tokenizer(\n",
    "        full_text, \n",
    "        truncation=True, \n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    format_instruction, \n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Entrenamiento (configuraciÃ³n para CPU)\n",
    "# -----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-tutor3\",\n",
    "    per_device_train_batch_size=2,      # Phi-3.5 es mÃ¡s pequeÃ±o, podemos usar batch_size=2\n",
    "    gradient_accumulation_steps=8,      \n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,                 # 3 Ã©pocas es razonable para Phi-3.5\n",
    "    fp16=False,                         \n",
    "    save_steps=100,\n",
    "    save_total_limit=2,                 \n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=50,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    dataloader_num_workers=0,           \n",
    "    optim=\"adamw_torch\",                \n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",                   \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "print(\"Iniciando entrenamiento en CPU (esto puede ser LENTO)...\")\n",
    "trainer.train()\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Guardar adaptadores LoRA\n",
    "# -----------------------------\n",
    "model.save_pretrained(\"./lora-tutor3\")\n",
    "tokenizer.save_pretrained(\"./lora-tutor3\")\n",
    "print(\"âœ… Entrenamiento completado. Adaptadores guardados en ./lora-tutor3\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Ejemplo de inferencia (opcional)\n",
    "# -----------------------------\n",
    "print(\"\\n--- Prueba de inferencia ---\")\n",
    "model.eval()\n",
    "test_prompt = \"<|user|>\\nÂ¿CÃ³mo crear una lista en Java?<|end|>\\n<|assistant|>\\n\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee4ff3cf-3ea7-4297-a8bc-86e51dc0b1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuy-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando adaptadores LoRA...\n",
      "\n",
      "============================================================\n",
      "ðŸ¤– Tutor de ProgramaciÃ³n LoRA - Listo para responder\n",
      "============================================================\n",
      "\n",
      "Pregunta: ExplÃ­came quÃ© es una funciÃ³n en Java.\n",
      "Respuesta: ExplÃ­came quÃ© es una funciÃ³n en Java. Es un bloque de cÃ³digo encapsulado que puede ser llamado con su nombre y argumentos opcionales (tipo void).\n",
      "```java\n",
      "public static int sumar(int a, int b) { return a + b; } // Funcion para operaciÃ³n aritmetica simple.\n",
      "System.out.println(\"Suma: \" + sumar(2, 3));\n",
      "// Salida: Suma: 5\n",
      "```\n",
      "\n",
      "Pregunta: Â¿CÃ³mo puedo leer un archivo en Java?\n",
      "Respuesta: Â¿CÃ³mo puedo leer un archivo en Java? Se utiliza `FileReader` para lectura de texto y `BufferedReader` agrega bufferizaciÃ³n.\n",
      "\n",
      "```java\n",
      "import java.io.*; // Importar IO (Necesario)\n",
      "public class Lectura {\n",
      "    public static void main(String[] args) throws Exception {\n",
      "        BufferedReader br = new BufferedReader(new FileReader(\"archivo.txt\"));\n",
      "         System.out.println(br.readLine()); \n",
      "       } catch(IOException e){System.err.print(e);} finally{if(br!=null)try{br.close();}catch(Exception ignored)=>{}}\n",
      "}```\n",
      "\n",
      "Pregunta: Â¿QuÃ© es un bucle for?\n",
      "Respuesta: Â¿QuÃ© es un bucle for? Permite iterar sobre una colecciÃ³n (array, lista) o intervalo de nÃºmeros.\n",
      "\n",
      "```java\n",
      "for(int i = 0; i < n; ++i){} // Bucle basado en contador `n`.\n",
      "List<String> names = new ArrayList<>();\n",
      "names.forEach(name -> System.out.println(name)); // OrdenaciÃ³n funcional con streams.\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# 1. ConfiguraciÃ³n\n",
    "# -----------------------------\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\" \n",
    "lora_path = \"./lora-tutor3\"  \n",
    "\n",
    "# -----------------------------\n",
    "# 2. Cargar modelo base y adaptadores LoRA\n",
    "# -----------------------------\n",
    "print(\"Cargando modelo base...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16,      \n",
    "    device_map=\"cpu\",               \n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"Cargando adaptadores LoRA...\")\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "model.eval()  # Modo evaluaciÃ³n\n",
    "\n",
    "# -----------------------------\n",
    "# 3. FunciÃ³n de inferencia\n",
    "# -----------------------------\n",
    "def generar_respuesta(pregunta, max_tokens=300, temperature=0.7):\n",
    "    # Formato correcto para Phi-3.5\n",
    "    prompt = f\"<|user|>\\n{pregunta}<|end|>\\n<|assistant|>\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    # NO usar .to(\"cuda\") porque estamos en CPU\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    respuesta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extraer solo la respuesta del asistente\n",
    "    if \"<|assistant|>\" in respuesta_completa:\n",
    "        respuesta = respuesta_completa.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        respuesta = respuesta_completa\n",
    "    \n",
    "    return respuesta\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Ejemplos de uso\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ¤– Tutor de ProgramaciÃ³n LoRA - Listo para responder\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Ejemplo 1\n",
    "pregunta1 = \"ExplÃ­came quÃ© es una funciÃ³n en Java.\"\n",
    "print(f\"Pregunta: {pregunta1}\")\n",
    "print(f\"Respuesta: {generar_respuesta(pregunta1)}\\n\")\n",
    "\n",
    "# Ejemplo 2\n",
    "pregunta2 = \"Â¿CÃ³mo puedo leer un archivo en Java?\"\n",
    "print(f\"Pregunta: {pregunta2}\")\n",
    "print(f\"Respuesta: {generar_respuesta(pregunta2)}\\n\")\n",
    "\n",
    "# Ejemplo 3\n",
    "pregunta3 = \"Â¿QuÃ© es un bucle for?\"\n",
    "print(f\"Pregunta: {pregunta3}\")\n",
    "print(f\"Respuesta: {generar_respuesta(pregunta3)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927689a-d7b4-48d1-866f-124195901256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845335d8-d2c0-411f-a767-4ec86608d3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80f653851d94e74b408173d8d698fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando adaptadores LoRA...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘¤ Tu pregunta:  Â¿QuÃ© es un algoritmo?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Respuesta: Â¿QuÃ© es un algoritmo? Un conjunto de instrucciones o pasos para resolver problemas.\n",
      "\n",
      "```java\n",
      "public int suma(int a, int b) { return a + b; } // Algoritmo simple: Sumar dos nÃºmeros.\n",
      "```\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘¤ Tu pregunta:  Â¿CÃ³mo puedo ordenar una secuencia de nÃºmeros?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Respuesta: Â¿CÃ³mo puedo ordenar una secuencia de nÃºmeros? La clase `Collections` proporciona mÃ©todos estÃ¡ticos para colas y listas, como el `sort()` que utiliza TimSort (una variaciÃ³n del merge sort).\n",
      "```java\n",
      "import java.util.*;\n",
      "public class Ordenamiento {\n",
      "    public static void main(String[] args) {\n",
      "        List<Integer> numeros = Arrays.asList(34, 789, -12);\n",
      "        Collections.sort(numeros); // [âˆ’12, 34, 789]\n",
      "    }\n",
      "}```\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m respuesta\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     pregunta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mðŸ‘¤ Tu pregunta: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pregunta\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msalir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÂ¡Hasta luego! ðŸ‘‹\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# 1. ConfiguraciÃ³n\n",
    "# -----------------------------\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\" \n",
    "lora_path = \"./lora-tutor3\"  \n",
    "\n",
    "# -----------------------------\n",
    "# 2. Cargar modelo base y adaptadores LoRA\n",
    "# -----------------------------\n",
    "print(\"Cargando modelo base...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16,      \n",
    "    device_map=\"cpu\",               \n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"Cargando adaptadores LoRA...\")\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "model.eval()  # Modo evaluaciÃ³n\n",
    "\n",
    "# -----------------------------\n",
    "# 3. FunciÃ³n de inferencia\n",
    "# -----------------------------\n",
    "def generar_respuesta(pregunta, max_tokens=300, temperature=0.7):\n",
    "    # Formato correcto para Phi-3.5\n",
    "    prompt = f\"<|user|>\\n{pregunta}<|end|>\\n<|assistant|>\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    # NO usar .to(\"cuda\") porque estamos en CPU\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    respuesta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extraer solo la respuesta del asistente\n",
    "    if \"<|assistant|>\" in respuesta_completa:\n",
    "        respuesta = respuesta_completa.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        respuesta = respuesta_completa\n",
    "    \n",
    "    return respuesta\n",
    "while True:\n",
    "    pregunta = input(\"\\nðŸ‘¤ Tu pregunta: \").strip()\n",
    "    \n",
    "    if pregunta.lower() in ['salir', 'exit', 'quit']:\n",
    "        print(\"Â¡Hasta luego! ðŸ‘‹\")\n",
    "        break\n",
    "    \n",
    "    if not pregunta:\n",
    "        continue\n",
    "    \n",
    "    print(f\"ðŸ¤– Respuesta: {generar_respuesta(pregunta)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
