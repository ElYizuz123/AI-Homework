{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de838f86-bd53-4daf-a839-825121461b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuy-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\chuy-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd15f4d-b821-4a55-8b88-d827a5f2bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Cargar dataset\n",
    "# -----------------------------\n",
    "dataset = load_dataset(\"json\", data_files=\"dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7319922-23b2-4153-bb50-2d79e598375a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or 'auto', 'balanced', 'balanced_low_0', 'sequential' but found None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:4788\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4787\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, maia, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: None",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/Phi-3.5-mini-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    605\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    606\u001b[0m     )\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    610\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:4790\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4788\u001b[0m         device_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mdevice(device_map)}\n\u001b[0;32m   4789\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[1;32m-> 4790\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   4791\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4792\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced_low_0\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequential\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_map\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4793\u001b[0m         )\n\u001b[0;32m   4794\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m   4795\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: When passing device_map as a string, the value needs to be a device name (e.g. cpu, cuda:0) or 'auto', 'balanced', 'balanced_low_0', 'sequential' but found None."
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 2. Cargar modelo base\n",
    "# -----------------------------\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"None\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff696da2-51df-4d52-8876-2658cbeae492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Configurar LoRA\n",
    "# -----------------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70515249-2533-4189-9c7b-84399918d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Preprocesar el dataset\n",
    "# -----------------------------\n",
    "def format_instruction(example):\n",
    "    prompt = f\"Instrucci√≥n: {example['instruction']}\\nRespuesta:\"\n",
    "    return tokenizer(prompt + example[\"response\"], truncation=True)\n",
    "\n",
    "tokenized = dataset.map(format_instruction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0f62a96-d18c-4a68-b9d8-270c529683e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuy-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\training_args.py:1636: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function MmBackward0 returned an invalid gradient at index 1 - expected device meta but got cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m\n\u001b[0;32m      4\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      5\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./lora-tutor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     no_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,           \u001b[38;5;66;03m# IMPORTANT√çSIMO\u001b[39;00m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     17\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     18\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     19\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     20\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2672\u001b[0m )\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2680\u001b[0m ):\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py:4071\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   4069\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 4071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\accelerate\\accelerator.py:2852\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2850\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2851\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2852\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    624\u001b[0m     )\n\u001b[1;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    843\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Function MmBackward0 returned an invalid gradient at index 1 - expected device meta but got cpu"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5. Entrenamiento\n",
    "# -----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-tutor\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=1,     # Pon 1 para no tardar horas\n",
    "    fp16=False,             # IMPORTANTE\n",
    "    bf16=False,\n",
    "    save_steps=500,\n",
    "    no_cuda=True,           # IMPORTANT√çSIMO\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38256eda-fd89-43fb-b334-552c87e83397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6. Guardar adaptadores LoRA\n",
    "# -----------------------------\n",
    "model.save_pretrained(\"./lora-tutor\")\n",
    "print(\"Entrenamiento completado. Adaptadores guardados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "105fa2c4-0eba-4c4f-9459-db262bcd0b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 23:49:36.125568: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-10 23:49:36.161729: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-10 23:49:37.187395: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fe365bf176418c8b3a56e09d36d827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,718,592 || all params: 3,825,798,144 || trainable%: 0.1233\n",
      "Par√°metros entrenables: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04dc3b60fa31458ea214f5747ad01e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "/home/yisus/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento en CPU (esto puede ser LENTO)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 5:44:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.821500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.902600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.837500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.427300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.349500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Entrenamiento completado. Adaptadores guardados en ./lora-tutor3\n",
      "\n",
      "--- Prueba de inferencia ---\n",
      "¬øC√≥mo crear una lista en Python? En Python, listas se implementa utilizando estructuras nativas o clases personalizadas. Aqu√≠ tienes un ejemplo simple en Python relacionado con listas.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Cargar dataset\n",
    "# -----------------------------\n",
    "dataset = load_dataset(\"json\", data_files=\"tutor_dataset2.jsonl\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Cargar modelo base (CPU optimizado)\n",
    "# -----------------------------\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Cargar modelo en CPU con dtype float32 (sin cuantizaci√≥n)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,  # Usar float32 para CPU\n",
    "    low_cpu_mem_usage=True,     # Optimizar uso de memoria\n",
    "    device_map=\"cpu\"            # Forzar CPU\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Configurar LoRA (ajustado para Phi-3.5)\n",
    "# -----------------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                        # Phi-3.5 mini es peque√±o, podemos usar r=8\n",
    "    lora_alpha=16,              \n",
    "    target_modules=[\"qkv_proj\", \"o_proj\"],  # M√≥dulos correctos para Phi-3.5\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(f\"Par√°metros entrenables: {model.print_trainable_parameters()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Preprocesar el dataset\n",
    "# -----------------------------\n",
    "def format_instruction(example):\n",
    "    # Formato espec√≠fico para Phi-3.5\n",
    "    prompt = f\"<|user|>\\n{example['instruction']}<|end|>\\n<|assistant|>\\n\"\n",
    "    full_text = prompt + example[\"response\"] + \"<|end|>\"\n",
    "    \n",
    "    # Tokenizar con longitud m√°xima\n",
    "    return tokenizer(\n",
    "        full_text, \n",
    "        truncation=True, \n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    format_instruction, \n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Entrenamiento (configuraci√≥n para CPU)\n",
    "# -----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-tutor3\",\n",
    "    per_device_train_batch_size=2,      # Phi-3.5 es m√°s peque√±o, podemos usar batch_size=2\n",
    "    gradient_accumulation_steps=8,      \n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,                 # 3 √©pocas es razonable para Phi-3.5\n",
    "    fp16=False,                         \n",
    "    save_steps=100,\n",
    "    save_total_limit=2,                 \n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=50,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    dataloader_num_workers=0,           \n",
    "    optim=\"adamw_torch\",                \n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",                   \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "print(\"Iniciando entrenamiento en CPU (esto puede ser LENTO)...\")\n",
    "trainer.train()\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Guardar adaptadores LoRA\n",
    "# -----------------------------\n",
    "model.save_pretrained(\"./lora-tutor3\")\n",
    "tokenizer.save_pretrained(\"./lora-tutor3\")\n",
    "print(\"‚úÖ Entrenamiento completado. Adaptadores guardados en ./lora-tutor3\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Ejemplo de inferencia (opcional)\n",
    "# -----------------------------\n",
    "print(\"\\n--- Prueba de inferencia ---\")\n",
    "model.eval()\n",
    "test_prompt = \"<|user|>\\n¬øC√≥mo crear una lista en Python?<|end|>\\n<|assistant|>\\n\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee4ff3cf-3ea7-4297-a8bc-86e51dc0b1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:24<00:00, 12.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando adaptadores LoRA...\n",
      "\n",
      "============================================================\n",
      "ü§ñ Tutor de Programaci√≥n LoRA - Listo para responder\n",
      "============================================================\n",
      "\n",
      "Pregunta: Expl√≠came qu√© es una funci√≥n en Python.\n",
      "Respuesta: Expl√≠came qu√© es una funci√≥n en Python. En el contexto del lenguaje de programaci√≥n Python, la palabra \"funci√≥n\" se refiere a un bloque compuesto y reutilizable de c√≥digo que realiza tareas espec√≠ficas o calculos predefinidos cuando se llama con ciertos argumentos (llamados par√°metros). Las funciones ayudan al desarrollo eficiente dividiendo las operaciones complejas en partes m√°s peque√±as y manejables, permitiendo as√≠ mejor legibilidad, modularizaci√≥n y mantenimiento del programa.\n",
      "\n",
      "Aqu√≠ hay algunas caracter√≠sticas clave sobre las funciones en Python:\n",
      "\n",
      "1. Definici√≥n: Una funci√≥n puede definirse usando la sintaxis `def` seguida por su nombre junto con los par√©ntesis para contener sus posibles argumentos entre ellos. Despu√©s viene dos puntos (:) y luego la lista de declaraciones dentro de l√≠neas separadas u otra estructura como indentado ra√≠z. Los comentarios pueden incluirse tambi√©n antes (`#`) de esta l√≠nea. Por ejemplo:\n",
      "```python\n",
      "def mi_funcion(parametro):  # Declara tu propia funcion aqu√≠\n",
      "    print(\"El valor pasado fue\", parametro)\n",
      "```\n",
      "2. Argumentos Opcionales/Valores Predeterminados: Puede proporcionar valores predeterminados para algunos de los par\n",
      "\n",
      "Pregunta: ¬øC√≥mo puedo leer un archivo en Python?\n",
      "Respuesta: ¬øC√≥mo puedo leer un archivo en Python? Para leer contenido de un archivo usando Python, puede usar el m√©todo `open()` junto con una instrucci√≥n `with`. Aqu√≠ hay un ejemplo simple:\n",
      "\n",
      "```python\n",
      "try:\n",
      "    with open('yourfile.txt', 'r') as file:  # reemplaza 'yourfile.txt' por la ruta/nombre real del tuyo archivo\n",
      "        content = file.read()\n",
      "except FileNotFoundError:\n",
      "    print(\"El archivo no fue encontrado.\")\n",
      "else:\n",
      "    print(content)\n",
      "```\n",
      "\n",
      "En este c√≥digo, primero intentamos abrir el archivo especificado utilizando `'r'` como modo (para lectura). La declaraci√≥n `with` asegura que se cierre autom√°ticamente y libere los recursos despu√©s de terminar nuestro bloque anidado sin necesidad de llamadas expl√≠citas al `.close()`. Luego, accedemos al contenido del archivo mediante el atributo `.read()`, lo cual guarda toda la informaci√≥n dentro del variable `content`. Finalmente, imprimimos ese contenido o manejamos cualquier error potencial relacionado con falta de archivos (`FileNotFoundError`). Ajuste las partes relevantes seg√∫n su caso particular para trabajar eficazmente con sus propios datos.\n",
      "\n",
      "Pregunta: ¬øQu√© es un bucle for?\n",
      "Respuesta: ¬øQu√© es un bucle for? Un \"bucle for\" (o simplemente 'for') se refiere a una estructura de control en programaci√≥n que permite ejecutar repetidamente un bloque de c√≥digo mientras se cumple cierta condici√≥n. Es particularmente √∫til cuando necesitas iterar sobre elementos dentro de colecciones, como listas, conjuntos o diccionarios.\n",
      "\n",
      "Aqu√≠ tienes la forma b√°sica y funcionalidad del bucle `for` en Python:\n",
      "\n",
      "```python\n",
      "para elemento en collection_name:  # La sintaxis para el bucle for general\n",
      "    instrucci√≥n(s)           # El conjunto de comandos/instrucciones por realizar con cada iteraci√≥n\n",
      "```\n",
      "\n",
      "Ejemplo pr√°ctico usando lista en Python:\n",
      "\n",
      "```python\n",
      "mi_lista = [10, 20, 30, 40, 50]\n",
      "para valor in mi_lista:              # Iteramos sobre los valores en la lista\n",
      "    print(\"El n√∫mero actual es \", valor)   # Imprimimos el valor durante cada ciclo\n",
      "```\n",
      "\n",
      "Salida esperada:\n",
      "```\n",
      "El n√∫mero actual es 10\n",
      "El n√∫mero actual es 20\n",
      "El n√∫mero actual es 30\n",
      "El n√∫mero actual es 40\n",
      "El n√∫mero actual es 50\n",
      "```\n",
      "En este ejemplo, as\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Configuraci√≥n\n",
    "# -----------------------------\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\" \n",
    "lora_path = \"./lora-tutor3\"  \n",
    "\n",
    "# -----------------------------\n",
    "# 2. Cargar modelo base y adaptadores LoRA\n",
    "# -----------------------------\n",
    "print(\"Cargando modelo base...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float32,      \n",
    "    device_map=\"cpu\",               \n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"Cargando adaptadores LoRA...\")\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "model.eval()  # Modo evaluaci√≥n\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Funci√≥n de inferencia\n",
    "# -----------------------------\n",
    "def generar_respuesta(pregunta, max_tokens=300, temperature=0.7):\n",
    "    # Formato correcto para Phi-3.5\n",
    "    prompt = f\"<|user|>\\n{pregunta}<|end|>\\n<|assistant|>\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    # NO usar .to(\"cuda\") porque estamos en CPU\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    respuesta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extraer solo la respuesta del asistente\n",
    "    if \"<|assistant|>\" in respuesta_completa:\n",
    "        respuesta = respuesta_completa.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        respuesta = respuesta_completa\n",
    "    \n",
    "    return respuesta\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Ejemplos de uso\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ü§ñ Tutor de Programaci√≥n LoRA - Listo para responder\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Ejemplo 1\n",
    "pregunta1 = \"Expl√≠came qu√© es una funci√≥n en Java.\"\n",
    "print(f\"Pregunta: {pregunta1}\")\n",
    "print(f\"Respuesta: {generar_respuesta(pregunta1)}\\n\")\n",
    "\n",
    "# Ejemplo 2\n",
    "pregunta2 = \"¬øC√≥mo puedo leer un archivo en Java?\"\n",
    "print(f\"Pregunta: {pregunta2}\")\n",
    "print(f\"Respuesta: {generar_respuesta(pregunta2)}\\n\")\n",
    "\n",
    "# Ejemplo 3\n",
    "pregunta3 = \"¬øQu√© es un bucle for?\"\n",
    "print(f\"Pregunta: {pregunta3}\")\n",
    "print(f\"Respuesta: {generar_respuesta(pregunta3)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927689a-d7b4-48d1-866f-124195901256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845335d8-d2c0-411f-a767-4ec86608d3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f111b5aa81bb46e193e0ac15e3086382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando adaptadores LoRA...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üë§ Tu pregunta:  ¬øPuedes decirme como imprimir un Hola Mundo! en java ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Respuesta: ¬øPuedes decirme como imprimir un Hola Mundo! en java ? ¬°Claro! Aqu√≠ tienes el c√≥digo b√°sico para crear una clase simple en Java que, cuando se ejecute, mostrar√° \"Hello World\" en la consola:\n",
      "\n",
      "```java\n",
      "public class HelloWorld {\n",
      "    public static void main(String[] args) {\n",
      "        System.out.println(\"Hello World\");\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "Para compilar y ejecutar este programa de Java:\n",
      "\n",
      "1. Gu√°rdalo con extensi√≥n `.java`, por ejemplo `HelloWorld.java`.\n",
      "2. Usa tu terminal o s√≠mbolo del sistema e inicia el Javac Compiler escribiendo `javac HelloWorld.java`. Esto generar√° un archivo bytecode llamado `HelloWorld.class` si todo est√° bien.\n",
      "3. Ejecuta luego el int√©rprete Java usando `java HelloWorld` desde la l√≠nea de comandos. Deber√≠as ver el mensaje \"Hello World\" impreso a la salida.\n",
      "\n",
      "Recuerda tener instalado el JDK (Java Development Kit), ya que es necesario para desarrollar aplicaciones Java. Si no lo has configurado previamente, puedes encontrarte utilizando el comando alternativo `java -version` para confirmar que Java est√© correctamente a√±adido al PATH de tu computadora.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m respuesta\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     pregunta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43müë§ Tu pregunta: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pregunta\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msalir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m¬°Hasta luego! üëã\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Configuraci√≥n\n",
    "# -----------------------------\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\" \n",
    "lora_path = \"./lora-tutor\"  \n",
    "\n",
    "# -----------------------------\n",
    "# 2. Cargar modelo base y adaptadores LoRA\n",
    "# -----------------------------\n",
    "print(\"Cargando modelo base...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float32,      \n",
    "    device_map=\"cpu\",               \n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"Cargando adaptadores LoRA...\")\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "model.eval()  # Modo evaluaci√≥n\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Funci√≥n de inferencia\n",
    "# -----------------------------\n",
    "def generar_respuesta(pregunta, max_tokens=300, temperature=0.7):\n",
    "    # Formato correcto para Phi-3.5\n",
    "    prompt = f\"<|user|>\\n{pregunta}<|end|>\\n<|assistant|>\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    # NO usar .to(\"cuda\") porque estamos en CPU\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    respuesta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extraer solo la respuesta del asistente\n",
    "    if \"<|assistant|>\" in respuesta_completa:\n",
    "        respuesta = respuesta_completa.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        respuesta = respuesta_completa\n",
    "    \n",
    "    return respuesta\n",
    "while True:\n",
    "    pregunta = input(\"\\nüë§ Tu pregunta: \").strip()\n",
    "    \n",
    "    if pregunta.lower() in ['salir', 'exit', 'quit']:\n",
    "        print(\"¬°Hasta luego! üëã\")\n",
    "        break\n",
    "    \n",
    "    if not pregunta:\n",
    "        continue\n",
    "    \n",
    "    print(f\"ü§ñ Respuesta: {generar_respuesta(pregunta)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e899ceb-7cc9-4c79-b90a-a3dab5122023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 23:33:40.467564: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-10 23:33:42.893979: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-10 23:33:46.763411: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento LoRA con Qwen 2.5-0.5B\n",
      "======================================================================\n",
      "\n",
      "Cargando dataset...\n",
      " Dataset cargado: 500 ejemplos\n",
      "\n",
      "ü§ñ Cargando modelo: Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Cargando modelo base...\n",
      " Modelo cargado en CPU con dtype float16\n",
      "\n",
      "üîß Configurando LoRA...\n",
      " LoRA configurado\n",
      "trainable params: 2,162,688 || all params: 496,195,456 || trainable%: 0.4359\n",
      "\n",
      "üìù Preprocesando dataset...\n",
      "‚úÖ Dataset tokenizado: 500 ejemplos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è  Configurando entrenamiento...\n",
      "\n",
      "üèãÔ∏è  Creando Trainer...\n",
      "\n",
      "======================================================================\n",
      " INICIANDO ENTRENAMIENTO\n",
      "======================================================================\n",
      "  ADVERTENCIA: En CPU esto puede tardar 30 min - 2 horas\n",
      "    dependiendo de tu procesador y tama√±o del dataset\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yisus/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"Entrenamiento LoRA con Qwen 2.5-0.5B\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Cargar dataset\n",
    "# -----------------------------\n",
    "print(\"\\nCargando dataset...\")\n",
    "dataset = load_dataset(\"json\", data_files=\"tutor_dataset2.jsonl\")\n",
    "print(f\" Dataset cargado: {len(dataset['train'])} ejemplos\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Cargar modelo base LIGERO\n",
    "# -----------------------------\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "print(f\"\\nü§ñ Cargando modelo: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Configurar padding token si no existe\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"‚öôÔ∏è  Cargando modelo base...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,      \n",
    "    device_map=\"cpu\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Liberar memoria\n",
    "gc.collect()\n",
    "print(f\" Modelo cargado en CPU con dtype float16\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Configurar LoRA (optimizado para Qwen)\n",
    "# -----------------------------\n",
    "print(\"\\nüîß Configurando LoRA...\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                           \n",
    "    lora_alpha=32,                  \n",
    "    target_modules=[\n",
    "        \"q_proj\", \n",
    "        \"k_proj\", \n",
    "        \"v_proj\", \n",
    "        \"o_proj\"\n",
    "    ],                              \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\" LoRA configurado\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Preprocesar el dataset\n",
    "# -----------------------------\n",
    "print(\"\\nüìù Preprocesando dataset...\")\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"\n",
    "    Formato de chat para Qwen 2.5:\n",
    "    <|im_start|>user\n",
    "    {pregunta}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    {respuesta}<|im_end|>\n",
    "    \"\"\"\n",
    "    prompt = f\"<|im_start|>user\\n{example['instruction']}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    full_text = prompt + example[\"response\"] + \"<|im_end|>\"\n",
    "    \n",
    "    # Tokenizar con longitud controlada\n",
    "    return tokenizer(\n",
    "        full_text, \n",
    "        truncation=True, \n",
    "        max_length=512,             # L√≠mite razonable para CPU\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    format_instruction, \n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizando ejemplos\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset tokenizado: {len(tokenized['train'])} ejemplos\")\n",
    "\n",
    "# Liberar memoria\n",
    "gc.collect()\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Configuraci√≥n de entrenamiento\n",
    "# -----------------------------\n",
    "print(\"\\n‚öôÔ∏è  Configurando entrenamiento...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-qwen-tutor\",\n",
    "    \n",
    "    # --- Batch y optimizaci√≥n ---\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-4,\n",
    "    warmup_steps=50,\n",
    "    max_grad_norm=1.0,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    # --- Logging y barra de progreso ---\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,      # imprime inmediatamente\n",
    "    report_to=\"none\",             # no usa wandb/tensorboard, pero NO afecta tqdm\n",
    "    disable_tqdm=False,           # <- ACTIVA la barra de progreso!\n",
    "\n",
    "    # --- Guardado ---\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"steps\",\n",
    "\n",
    "    # --- Otros ---\n",
    "    fp16=False,                   # NO usar float16 en CPU\n",
    "    bf16=False,\n",
    "    dataloader_num_workers=0,\n",
    "    eval_strategy=\"no\",\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Crear Trainer\n",
    "# -----------------------------\n",
    "print(\"\\nüèãÔ∏è  Creando Trainer...\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. ENTRENAR\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" INICIANDO ENTRENAMIENTO\")\n",
    "print(\"=\"*70)\n",
    "print(\"  ADVERTENCIA: En CPU esto puede tardar 30 min - 2 horas\")\n",
    "print(\"    dependiendo de tu procesador y tama√±o del dataset\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"\\n Entrenamiento completado!\")\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n  Entrenamiento interrumpido por el usuario\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n Error durante el entrenamiento: {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Guardar modelo\n",
    "# -----------------------------\n",
    "print(\"\\nüíæ Guardando adaptadores LoRA...\")\n",
    "model.save_pretrained(\"./lora-qwen-tutor\")\n",
    "tokenizer.save_pretrained(\"./lora-qwen-tutor\")\n",
    "print(\"‚úÖ Adaptadores guardados en: ./lora-qwen-tutor\")\n",
    "\n",
    "# Liberar memoria\n",
    "del model\n",
    "del trainer\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" PROCESO COMPLETADO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e9b03ea-a31e-4621-9173-dcc3585cf900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c43e8a0f534804aab4ba95d41136f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1ef3914a7b4e97ba8b3fc588789161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e938cfa94899433d915768ea8f133ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2de425730244d3e865514138949a732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34229a2b3e8742a8aaf07d7f0828bdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,204,352 || all params: 3,836,283,904 || trainable%: 0.3963\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c60c4e0bb348f7b3780e160330e01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Your setup doesn't support bf16/gpu.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 75\u001b[0m\n\u001b[1;32m     70\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(format_instruction, remove_columns\u001b[38;5;241m=\u001b[39mdataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# 6. Entrenamiento (Configuraci√≥n GPU AMD)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./lora-tutor-phi35\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tu GPU aguanta batch m√°s grande (m√°s r√°pido)\u001b[39;49;00m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbf16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# Usar BFloat16 en lugar de FP16 (mejor en AMD)\u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.03\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madamw_torch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader_pin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Acelera transferencia RAM -> VRAM\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     93\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     94\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     95\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     96\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     97\u001b[0m )\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ Iniciando entrenamiento en GPU AMD...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<string>:135\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/transformers/training_args.py:1747\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                     error_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You need Ampere+ GPU with cuda>=11.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1746\u001b[0m                 \u001b[38;5;66;03m# gpu\u001b[39;00m\n\u001b[0;32m-> 1747\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_message)\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16:\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt most one of fp16 and bf16 can be True, but not both\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Your setup doesn't support bf16/gpu."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Configuraci√≥n de Hardware\n",
    "# -----------------------------\n",
    "# Detectar si hay GPU disponible (CUDA funciona como alias para ROCm en PyTorch)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Cargar dataset\n",
    "# -----------------------------\n",
    "dataset = load_dataset(\"json\", data_files=\"tutor_dataset2.jsonl\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Cargar modelo base (Optimizado para GPU)\n",
    "# -----------------------------\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Cargar en GPU directamente usando bfloat16 (mejor para AMD serie 6000/7000)\n",
    "# Phi-3.5 es peque√±o (~4GB en fp16), cabe sobrado en tu VRAM sin cuantizar a 4 bits.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,     # bfloat16 es m√°s estable y r√°pido en GPUs modernas\n",
    "    device_map=\"auto\",              # PyTorch distribuir√° el modelo en la GPU\n",
    "    attn_implementation=\"eager\"     # Compatibilidad general\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Configurar LoRA\n",
    "# -----------------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                           # Aumentamos a 16 ya que tienes buena GPU\n",
    "    lora_alpha=32,                  \n",
    "    # Agregamos m√°s m√≥dulos para un aprendizaje m√°s profundo\n",
    "    target_modules=[\"qkv_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Preprocesar el dataset\n",
    "# -----------------------------\n",
    "def format_instruction(example):\n",
    "    prompt = f\"<|user|>\\n{example['instruction']}<|end|>\\n<|assistant|>\\n\"\n",
    "    full_text = prompt + example[\"response\"] + \"<|end|>\"\n",
    "    return tokenizer(\n",
    "        full_text, \n",
    "        truncation=True, \n",
    "        max_length=1024, # Aumentamos contexto ya que tienes memoria de sobra\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(format_instruction, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Entrenamiento (Configuraci√≥n GPU AMD)\n",
    "# -----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-tutor-phi35\",\n",
    "    per_device_train_batch_size=8,  # Tu GPU aguanta batch m√°s grande (m√°s r√°pido)\n",
    "    gradient_accumulation_steps=2,   \n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch\",            \n",
    "    max_grad_norm=1.0,\n",
    "    report_to=\"none\",\n",
    "    dataloader_pin_memory=True      # Acelera transferencia RAM -> VRAM\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "print(\"üöÄ Iniciando entrenamiento en GPU AMD...\")\n",
    "trainer.train()\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Guardar adaptadores\n",
    "# -----------------------------\n",
    "final_path = \"./lora-tutor-final\"\n",
    "model.save_pretrained(final_path)\n",
    "tokenizer.save_pretrained(final_path)\n",
    "print(f\"‚úÖ Adaptadores guardados en {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1c5a983-ccb1-4933-b1fb-ed4a060fedf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.5 | packaged by conda-forge | (main, Aug  8 2024, 18:36:51) [GCC 12.4.0]\n",
      "PyTorch version: 2.4.1+rocm6.0\n",
      "------------------------------\n",
      "‚ùå No se detect√≥ ninguna GPU. PyTorch est√° usando CPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ ROCm/CUDA est√° activo y funcionando.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Tarjeta gr√°fica detectada: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Prueba de c√°lculo\n",
    "    try:\n",
    "        x = torch.randn(1024, 1024).to(device)\n",
    "        y = torch.randn(1024, 1024).to(device)\n",
    "        z = torch.matmul(x, y)\n",
    "        print(\"‚úÖ C√°lculo matricial (1024x1024) completado exitosamente.\")\n",
    "        \n",
    "        if torch.cuda.is_bf16_supported():\n",
    "            print(\"‚úÖ Soporte para bfloat16 confirmado (Ideal para entrenar).\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Tu tarjeta usar√° float16 est√°ndar.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error al realizar c√°lculos: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå No se detect√≥ ninguna GPU. PyTorch est√° usando CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d315c10a-78e9-4975-8243-1ea5ff6bd75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIP version: 6.0.32830-d62f6a171\n",
      "CUDA available: False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No HIP GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHIP version:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mhip)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/cuda/__init__.py:435\u001b[0m, in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mdevice\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Context-manager that changes the selected device.\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m        device (torch.device or int): device index to select. It's a no-op if\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m            this argument is a negative integer or ``None``.\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, device: Any):\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/cuda/__init__.py:465\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_device\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Set the current device.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m    Usage of this function is discouraged in favor of :any:`device`. In most\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m    cases it's better to use ``CUDA_VISIBLE_DEVICES`` environmental variable.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m        device (torch.device or int): selected device. This function is a no-op\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m            if this argument is negative.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device)\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/torch/cuda/__init__.py:314\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     )\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No HIP GPUs are available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"HIP version:\", torch.version.hip)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "215af404-d61c-4261-9c7f-4f70b0253af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.6.0+rocm6.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: /home/yisus/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, pytorch-triton-rocm, setuptools, sympy, typing-extensions\n",
      "Required-by: accelerate, peft, torchaudio, torchvision\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4392bd5c-ac0b-463c-82a2-14f415301983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versi√≥n de PyTorch: 2.4.1+rocm6.0\n",
      "¬øROCm disponible?: True\n",
      "torch.cuda.is_available(): False\n",
      "Nombre del dispositivo: No detectado\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Versi√≥n de PyTorch:\", torch.__version__)\n",
    "print(\"¬øROCm disponible?:\", torch.version.hip is not None)\n",
    "print(\"torch.cuda.is_available():\", torch.cuda.is_available())\n",
    "print(\"Nombre del dispositivo:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No detectado\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f16d3a-cd0c-4c34-8a1a-d9c2365696cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
