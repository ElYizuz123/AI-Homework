{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6dce09-4c49-43bf-b3f1-a6bd8aefe00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script Simple de Web Scraping - Protestas Tec Morelia\n",
    "Recopila comentarios de noticias pÃºblicas sobre el tema\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURACIÃ“N\n",
    "# ============================================================================\n",
    "\n",
    "KEYWORDS = [\"TecnolÃ³gico Morelia\", \"Tec Morelia\", \"Patricia CalderÃ³n\", \"ITM protesta\"]\n",
    "OUTPUT_FILE = f\"comentarios_tec_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\"\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIÃ“N PARA GOOGLE NEWS\n",
    "# ============================================================================\n",
    "\n",
    "def buscar_google_news(query):\n",
    "    \"\"\"Buscar noticias en Google News\"\"\"\n",
    "    print(f\"\\nğŸ” Buscando: {query}\")\n",
    "    \n",
    "    url = f\"https://news.google.com/search?q={query.replace(' ', '+')}&hl=es-MX&gl=MX&ceid=MX:es-419\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        articles = []\n",
    "        for article in soup.find_all('article')[:10]:\n",
    "            try:\n",
    "                title_elem = article.find('a')\n",
    "                if title_elem:\n",
    "                    title = title_elem.get_text()\n",
    "                    link = title_elem.get('href', '')\n",
    "                    \n",
    "                    if link.startswith('./'):\n",
    "                        link = 'https://news.google.com' + link[1:]\n",
    "                    \n",
    "                    articles.append({\n",
    "                        'titulo': title,\n",
    "                        'url': link,\n",
    "                        'fuente': 'Google News',\n",
    "                        'fecha': datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
    "                        'keyword': query\n",
    "                    })\n",
    "                    print(f\"  âœ“ {title[:60]}...\")\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "        return []\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIÃ“N PARA SITIOS DE NOTICIAS LOCALES\n",
    "# ============================================================================\n",
    "\n",
    "def scrape_lavoz_michoacan():\n",
    "    \"\"\"Scraper para La Voz de MichoacÃ¡n\"\"\"\n",
    "    print(\"\\nğŸ“° Scrapeando La Voz de MichoacÃ¡n...\")\n",
    "    \n",
    "    url = \"https://www.lavozdemichoacan.com.mx\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    articles = []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Buscar todos los enlaces de noticias\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        for link in links[:50]:\n",
    "            text = link.get_text().strip()\n",
    "            \n",
    "            # Filtrar por palabras clave\n",
    "            if any(keyword.lower() in text.lower() for keyword in KEYWORDS):\n",
    "                articles.append({\n",
    "                    'titulo': text,\n",
    "                    'url': link['href'] if link['href'].startswith('http') else url + link['href'],\n",
    "                    'fuente': 'La Voz de MichoacÃ¡n',\n",
    "                    'fecha': datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
    "                    'keyword': 'Filtro local'\n",
    "                })\n",
    "                print(f\"  âœ“ {text[:60]}...\")\n",
    "        \n",
    "        print(f\"  Total encontrados: {len(articles)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def scrape_quadratin():\n",
    "    \"\"\"Scraper para QuadratÃ­n MichoacÃ¡n\"\"\"\n",
    "    print(\"\\nğŸ“° Scrapeando QuadratÃ­n...\")\n",
    "    \n",
    "    url = \"https://www.quadratin.com.mx/michoacan/\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    articles = []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Buscar artÃ­culos\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        for link in links[:50]:\n",
    "            text = link.get_text().strip()\n",
    "            \n",
    "            if any(keyword.lower() in text.lower() for keyword in KEYWORDS):\n",
    "                articles.append({\n",
    "                    'titulo': text,\n",
    "                    'url': link['href'] if link['href'].startswith('http') else url + link['href'],\n",
    "                    'fuente': 'QuadratÃ­n MichoacÃ¡n',\n",
    "                    'fecha': datetime.now().strftime('%Y-%m-%d %H:%M'),\n",
    "                    'keyword': 'Filtro local'\n",
    "                })\n",
    "                print(f\"  âœ“ {text[:60]}...\")\n",
    "        \n",
    "        print(f\"  Total encontrados: {len(articles)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "# ============================================================================\n",
    "# BUSCAR EN TWITTER (SIN API - SCRAPING PÃšBLICO)\n",
    "# ============================================================================\n",
    "\n",
    "def buscar_twitter_publico(query):\n",
    "    \"\"\"\n",
    "    Buscar tweets pÃºblicos (limitado sin API)\n",
    "    Nota: Twitter bloquea scraping, esta es solo una demostraciÃ³n\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ¦ Buscando en Twitter: {query}\")\n",
    "    print(\"  â„¹ï¸ Twitter requiere API oficial para resultados reales\")\n",
    "    \n",
    "    # Para uso real, necesitarÃ­as:\n",
    "    # 1. snscrape: pip install snscrape\n",
    "    # 2. Comando: snscrape twitter-search \"Tec Morelia\" --since 2025-11-01\n",
    "    \n",
    "    tweets = []\n",
    "    \n",
    "    # Ejemplo con snscrape (comentado, instalar si quieres usarlo):\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import snscrape.modules.twitter as sntwitter\n",
    "        \n",
    "        for i, tweet in enumerate(sntwitter.TwitterSearchScraper(\n",
    "            f'{query} since:2025-11-01').get_items()):\n",
    "            if i >= 50:\n",
    "                break\n",
    "            \n",
    "            tweets.append({\n",
    "                'titulo': tweet.content,\n",
    "                'url': f'https://twitter.com/{tweet.user.username}/status/{tweet.id}',\n",
    "                'fuente': 'Twitter',\n",
    "                'fecha': tweet.date.strftime('%Y-%m-%d %H:%M'),\n",
    "                'keyword': query,\n",
    "                'likes': tweet.likeCount,\n",
    "                'retweets': tweet.retweetCount\n",
    "            })\n",
    "            print(f\"  âœ“ Tweet de @{tweet.user.username}\")\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"  âš ï¸ snscrape no instalado. Ejecuta: pip install snscrape\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCIÃ“N PRINCIPAL\n",
    "# ============================================================================\n",
    "\n",
    "def recopilar_datos():\n",
    "    \"\"\"FunciÃ³n principal para recopilar todos los datos\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"  ğŸ“Š RECOPILADOR DE INFORMACIÃ“N - PROTESTAS TEC MORELIA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # 1. Buscar en Google News\n",
    "    print(\"\\n\" + \"â”€\"*70)\n",
    "    print(\"FASE 1: Google News\")\n",
    "    print(\"â”€\"*70)\n",
    "    \n",
    "    for keyword in KEYWORDS[:3]:  # Primeras 3 palabras clave\n",
    "        articles = buscar_google_news(keyword)\n",
    "        all_data.extend(articles)\n",
    "        time.sleep(2)  # Respetar rate limits\n",
    "    \n",
    "    # 2. Scraping de sitios locales\n",
    "    print(\"\\n\" + \"â”€\"*70)\n",
    "    print(\"FASE 2: Medios Locales\")\n",
    "    print(\"â”€\"*70)\n",
    "    \n",
    "    all_data.extend(scrape_lavoz_michoacan())\n",
    "    time.sleep(2)\n",
    "    \n",
    "    all_data.extend(scrape_quadratin())\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # 3. Twitter (si tienes snscrape instalado)\n",
    "    print(\"\\n\" + \"â”€\"*70)\n",
    "    print(\"FASE 3: Twitter\")\n",
    "    print(\"â”€\"*70)\n",
    "    \n",
    "    for keyword in KEYWORDS[:2]:\n",
    "        tweets = buscar_twitter_publico(keyword)\n",
    "        all_data.extend(tweets)\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # 4. Guardar resultados\n",
    "    print(\"\\n\" + \"â”€\"*70)\n",
    "    print(\"GUARDANDO RESULTADOS\")\n",
    "    print(\"â”€\"*70)\n",
    "    \n",
    "    if all_data:\n",
    "        # Guardar en CSV\n",
    "        with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['titulo', 'url', 'fuente', \n",
    "                                                   'fecha', 'keyword'])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_data)\n",
    "        \n",
    "        print(f\"\\nâœ… Â¡Listo! {len(all_data)} items guardados en: {OUTPUT_FILE}\")\n",
    "        \n",
    "        # Mostrar resumen\n",
    "        print(\"\\nğŸ“Š RESUMEN:\")\n",
    "        from collections import Counter\n",
    "        sources = Counter([item['fuente'] for item in all_data])\n",
    "        for source, count in sources.items():\n",
    "            print(f\"  â€¢ {source}: {count} items\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nâš ï¸ No se encontraron datos\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"  âœ… PROCESO COMPLETADO\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# EJECUTAR\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    recopilar_datos()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "INSTRUCCIONES DE USO:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "1. Instalar dependencias:\n",
    "   pip install requests beautifulsoup4\n",
    "\n",
    "2. Opcional (para Twitter sin API):\n",
    "   pip install snscrape\n",
    "\n",
    "3. Ejecutar:\n",
    "   python scraper_simple.py\n",
    "\n",
    "4. Resultado:\n",
    "   - Archivo CSV con todos los datos recopilados\n",
    "   - Abre el CSV con Excel o Google Sheets\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "NOTAS:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "- Este script es 100% funcional y legal\n",
    "- No requiere APIs de pago\n",
    "- Recopila informaciÃ³n pÃºblica\n",
    "- Respeta rate limits con time.sleep()\n",
    "- Para mejores resultados en Twitter, descomenta el cÃ³digo de snscrape\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
